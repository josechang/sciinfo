@article{Hedbrant20162206,
title = "API REST and SOAP,
keywords = "SOAP Web-services ",
Retrieved from https://bitbucket.org/nordron/nordron-sciinfo
}

@article{Kochanov201615,
title = "\{HITRAN\} Application Programming Interface (HAPI): A comprehensive approach to working with spectroscopic data ",
journal = "Journal of Quantitative Spectroscopy and Radiative Transfer ",
volume = "177",
number = "",
pages = "15 - 30",
year = "2016",
note = "\{XVIIIth\} Symposium on High Resolution Molecular Spectroscopy (HighRus-2015), Tomsk, Russia ",
issn = "0022-4073",
doi = "http://dx.doi.org/10.1016/j.jqsrt.2016.03.005",
url = "http://www.sciencedirect.com/science/article/pii/S0022407315302466",
author = "R.V. Kochanov and I.E. Gordon and L.S. Rothman and P. Wcisło and C. Hill and J.S. Wilzewski",
keywords = "HITRAN",
keywords = "Application Programming Interface",
keywords = "API",
keywords = "Spectra simulation",
keywords = "HITRANonline ",
abstract = "Abstract The \{HITRAN\} Application Programming Interface (HAPI) is presented. 
\{HAPI\} is a free Python library, which extends the capabilities of the \{HITRANonline\} interface (www.hitran.org) and can be used to filter and process the structured spectroscopic data. 
\{HAPI\} incorporates a set of tools for spectra simulation accounting for the temperature, pressure, optical path length, and instrument properties. 
\{HAPI\} is aimed to facilitate the spectroscopic data analysis and the spectra simulation based on the line-by-line data, such as from the \{HITRAN\} database [JQSRT (2013) 130, 4–50], allowing the usage of the non-Voigt line profile parameters, custom temperature and pressure dependences, and partition sums. 
The \{HAPI\} functions allow the user to control the spectra simulation and data filtering process via a set of the function parameters. 
\{HAPI\} can be obtained at its homepage www.hitran.org/hapi. "
}

@incollection{Lazarinis2015,
abstract = {Metadata is data about other data. This chapter introduces some of the best-known metadata schemes and discusses their importance in the description of digital objects.},
author = {Lazarinis, Fotis},
booktitle = {Cataloguing and Classification},
doi = {10.1016/B978-0-08-100161-5.00012-9},
isbn = {9780081001615},
pages = {225--232},
title = {{Metadata}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9780081001615000129},
year = {2015}
}
@article{Sauri2009,
abstract = {Recent work in computational linguistics points out the need for systems to be sensitive to the veracity or factuality of events as mentioned in text; that is, to recognize whether events are presented as corresponding to actual situations in the world, situations that have not happened, or situations of uncertain interpretation. Event factuality is an important aspect of the representation of events in discourse, but the annotation of such information poses a representational challenge, largely because factuality is expressed through the interaction of numerous linguistic markers and constructions. Many of these markers are already encoded in existing corpora, albeit in a somewhat fragmented way. In this article, we present FactBank, a corpus annotated with information concerning the factuality of events. Its annotation has been carried out from a descriptive framework of factuality grounded on both theoretical findings and data analysis. FactBank is built on top of TimeBank, adding to it an additional level of semantic information.},
author = {Saur??, Roser and Pustejovsky, James},
doi = {10.1007/s10579-009-9089-9},
isbn = {1057900990},
issn = {1574020X},
journal = {Language Resources and Evaluation},
keywords = {Certainty,Corpus creation,Event factuality,Modality,Subjectivity analysis,TimeBank},
number = {3},
pages = {227--268},
publisher = {Springer Netherlands},
title = {{Factbank: A corpus annotated with event factuality}},
url = {http://link.springer.com/10.1007/s10579-009-9089-9},
volume = {43},
year = {2009}
}
@article{NISO2012,
abstract = {Defines fifteen metadata elements for resource description in a cross-disciplinary information environment.},
author = {(NISO), National Information Standards Organization},
issn = {1041-5635},
title = {{The Dublin Core metadata element set}},
volume = {Version 1.},
year = {2012}
}
@article{Agerri2014,
abstract = {Requirements in computational power have grown dramatically in recent years. This is also the case in many language processing tasks, due to the overwhelming and ever increasing amount of textual information that must be processed in a reasonable time frame. This scenario has led to a paradigm shift in the computing architectures and large-scale data processing strategies used in the Natural Language Processing field. In this paper we present a new distributed architecture and technology for scaling up text analysis running a complete chain of linguistic processors on several virtual machines. Furthermore, we also describe a series of experiments carried out with the goal of analyzing the scaling capabilities of the language processing pipeline used in this setting. We explore the use of Storm in a new approach for scalable distributed language processing across multiple machines and evaluate its effectiveness and efficiency when processing documents on a medium and large scale. The experiments have shown that there is a big room for improvement regarding language processing performance when adopting parallel architectures, and that we might expect even better results with the use of large clusters with many processing nodes.},
author = {Agerri, Rodrigo and Artola, Xabier and Beloki, Zuhaitz and Rigau, German and Soroa, Aitor},
doi = {10.1016/j.knosys.2014.11.007},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Big data,Distributed NLP architectures,NLP tools,Natural Language Processing,Storm},
pages = {36--42},
title = {{Big data for Natural Language Processing: A streaming approach}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950705114003992},
volume = {79},
year = {2014}
}
@article{Wiebe2005,
abstract = {This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented.},
archivePrefix = {arXiv},
arxivId = {arXiv:1002.2562v1},
author = {Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
doi = {10.1007/s10579-005-7880-9},
eprint = {arXiv:1002.2562v1},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiebe, Cardie - Unknown - Annotating Expressions of Opinions and Emotions in Language.pdf:pdf},
isbn = {1574-020X},
issn = {1574020X},
journal = {Language Resources and Evaluation},
keywords = {Affect,Attitudes,Corpus annotation,Emotion,Natural language processing,Opinions,Sentiment,Subjectivity},
number = {2-3},
pages = {165--210},
pmid = {12658535},
title = {{Annotating expressions of opinions and emotions in language}},
volume = {39},
year = {2005}
}
@article{TunThuraThet2010,
abstract = {In this article, a method for automatic sentiment analysis of movie reviews is proposed, implemented and evaluated. In contrast to most studies that focus on determining only sentiment orientation (positive versus negative), the proposed method performs fine-grained analysis to determine both the sentiment orientation and sentiment strength of the reviewer towards various aspects of a movie. Sentences in review documents contain independent clauses that express different sentiments toward different aspects of a movie. The method adopts a linguistic approach of computing the sentiment of a clause from the prior sentiment scores assigned to individual words, taking into consideration the grammatical dependency structure of the clause. The prior sentiment scores of about 32,000 individual words are derived from SentiWordNet with the help of a subjectivity lexicon. Negation is delicately handled. The output sentiment scores can be used to identify the most positive and negative clauses or sentences with respect to particular movie aspects.},
author = {{Tun Thura Thet} and Na, J.-C. and Khoo, C. S. G.},
doi = {10.1177/0165551510388123},
isbn = {9781605588056},
issn = {0165-5515},
journal = {Journal of Information Science},
keywords = {dependency tree,discussion,movie reviews,sentiment analysis},
month = {dec},
number = {6},
pages = {823--848},
publisher = {Sage Publications, Inc.},
title = {{Aspect-based sentiment analysis of movie reviews on discussion boards}},
url = {http://jis.sagepub.com/cgi/doi/10.1177/0165551510388123},
volume = {36},
year = {2010}
}
@article{Theobald2008,
abstract = {Recent IR extensions to XML query languages such as Xpath 1.0 Full-Text or the NEXI query language of the INEX benchmark series reflect the emerging interest in IR-style ranked retrieval over semistruetured data. TopX is a top-k retrieval engine for text and semistruetured data. It terminates query execution as soon as it can safely determine the k top-ranked result elements according to a monotonic score aggregation function with respect to a multidimensional query. It efficiently supports vague search on both content- and structure-oriented query conditions for dynamic query relaxation with controllable influence on the result ranking. The main contributions of this paper unfold into four main points: (1) fully implemented models and algorithms for ranked XML retrieval with XPath Full-Text functionality, (2) efficient and effective top-k query processing for semistructured data, (3) support for integrating thesauri and ontologies with statistically quantified relationships among concepts, leveraged for word-sense disambiguation and query expansion, and (4) a comprehensive description of the TopX system, with performance experiments on large-scale corpora like TREC Terabyte and INEX Wikipedia.},
author = {Theobald, Martin and Bast, Holger and Majumdar, Debapriyo and Schenkel, Ralf and Weikum, Gerhard},
doi = {10.1007/s00778-007-0072-z},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Theobald et al. - 2007 - TopX efficient and versatile top-k query processing for semistructured data.pdf:pdf},
isbn = {9783885791973},
issn = {10668888},
journal = {VLDB Journal},
keywords = {Content- and structure-aware ranking,Cost-based index access scheduling,DB{\&}IR integration,Dynamic query expansion,Efficient XML full-text search,Probabilistic candidate pruning,Query processing},
month = {nov},
number = {1},
pages = {81--115},
publisher = {Springer-Verlag},
title = {{TopX: Efficient and versatile top-k query processing for semistructured data}},
url = {http://link.springer.com/10.1007/s00778-007-0072-z},
volume = {17},
year = {2008}
}
@article{Ruiz2011,
abstract = {This article is centred on analysing the state of the art of the conflation processes applied to geospatial databases (GDBs) from heterogeneous sources. The term conflation is used to describe the procedure for the integration of these different data, and conflation methods play an important role in systems for updating GDBs, derivation of new cartographic products, densification of digital elevation models, automatic features extraction and so on. In this article we define extensively each conflation process, its evaluation measures and its main application problems and present a classification of all conflation processes. Finally, we introduce a bibliography which the reader may find useful to further explore the field. It tries to serve as a starting point and direct the reader to characteristic research in this area.},
author = {Ruiz, Juan J. and Ariza, F. Javier and Ure{\~{n}}a, Manuel a. and Bl{\'{a}}zquez, Elidia B.},
doi = {10.1080/13658816.2010.519707},
isbn = {1365-8816},
issn = {1365-8816},
journal = {International Journal of Geographical Information Science},
keywords = {Conflation,Integration,data fusion,definitions},
month = {sep},
number = {9},
pages = {1439--1466},
title = {{Digital map conflation: a review of the process and a proposal for classification}},
url = {http://dx.doi.org/10.1080/13658816.2010.519707},
volume = {25},
year = {2011}
}
@article{Mammen2016,
abstract = {Indian classical music, including its two varieties, Carnatic and Hindustani music, has a rich music tradition and enjoys a wide audience from various parts of the world. The Carnatic music which is more popular in South India still continues to be uninfluenced by other music traditions and is one of the purest forms of Indian music. Like other music traditions, Carnatic music also has developed its musicography, out of which, a notation system called Sargam is most commonly practiced. This paper deals with development of a music representation or encoding system for the Sargam notation scheme which enables easy music notation storage, publishing, and retrieval using computers. This work follows a novel idea of developing a Unicode-based encoding logic and allows storage and easy retrieval of music notation files in a computer. As opposed to many existing music representation systems for western music notation, iSargam is the only music notation encoding system developed for Indian music notation.},
author = {Mammen, Stanly and Krishnamurthi, Ilango and Varma, A. Jalaja and Sujatha, G.},
doi = {10.1186/s13636-016-0083-z},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mammen et al. - 2016 - iSargam music notation representation for Indian Carnatic music.pdf:pdf},
issn = {1687-4722},
journal = {EURASIP Journal on Audio, Speech, and Music Processing},
keywords = {Computer music,Indian music,Music encoding,Music information retrieval,Music processing,Music representation},
month = {dec},
number = {1},
pages = {5},
publisher = {Springer International Publishing},
title = {{iSargam: music notation representation for Indian Carnatic music}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84958760963{\&}partnerID=tZOtx3y1},
volume = {2016},
year = {2016}
}
@article{Liu2012,
abstract = {Currently available web news retrieval systems face a number of problems in that web-based news retrieval requires the ability to quickly and accurately process and update a very large amount of data which are constantly being updated. In this paper, we present the development of an intelligent distributed web news retrieval system the goal of which is to accurately retrieve and organize the web news information. It includes: a novel optimized crawler algorithm whose fetching-speed is several times faster than that of the traditional crawler; a keen tag based extraction algorithm which can extract the data rich content with minimal manual effort and which also allows data to be classified as important or not important so that the crawler can revisit and update important data; a modified MapReduce improved by estimating the execution time of each subtask, which is proven to be able to reduce the number of the unusual tasks and shorten the whole job execution time. {\textcopyright} 2012 - IOS Press and the authors. All rights reserved.},
author = {Liu, James N K and Choi, K. C. and Chai, J. Y.},
doi = {10.3233/KES-2011-0237},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Choi, Chai - 2012 - Development of an intelligent distributed news retrieval system.pdf:pdf},
issn = {13272314},
journal = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
keywords = {Intelligent system,MapReduce,distributed news retrieval,web crawler},
month = {mar},
number = {2},
pages = {129--140},
publisher = {IOS Press},
title = {{Development of an intelligent distributed news retrieval system}},
url = {http://www.medra.org/servlet/aliasResolver?alias=iospress{\&}doi=10.3233/KES-2011-0237},
volume = {16},
year = {2012}
}
@article{Guenther2003,
abstract = {Metadata has taken on a new look with the advent of XML and digital resources. XML provides a new versatile structure for tagging and packaging metadata as the rapid proliferation of digital resources demands both rapidly produced descriptive data and the encoding of more types of metadata. Two emerging standards are attempting to harness these developments for library needs. The first is the Metadata Object and Description Schema (MODS), a MARC-compatible XML schema for encoding descriptive data. The second standard is the Metadata Encoding and Transmission Standard (METS), a highly flexible XML schema for packaging the descriptive metadata and various other important types of metadata needed to assure the use and preservation of digital resources.},
author = {Guenther, R and McCallum, S},
doi = {10.1002/bult.268},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guenther, McCallum - 2005 - New Metadata Standards for Digital Resources MODS and METS.pdf:pdf},
issn = {00954403},
journal = {Bulletin of the American Society for Information Science and Technology},
month = {jan},
number = {2},
pages = {12--15},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{New Metadata Standards for Digital Resources: MODS and METS}},
url = {http://www.asis.org/Bulletin/Dec-02/guenthermccallum.html},
volume = {29},
year = {2003}
}
@article{Grehan2002,
abstract = {This resource provides fairly in-depth information as to how search engines work, and by extension how they rank websites through search engine algorithms. This resource is very useful if you are unsure of the mechanics behind search engines.},
author = {Grehan, Mike},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grehan - Unknown - How Search Engines Work NOTE FROM THE AUTHOR(2).pdf:pdf},
journal = {Search Engine Marketing: The Essential Best Practice Guide},
pages = {57},
title = {{How Search Engines Work}},
year = {2002}
}
@article{Gabrilovich2007,
abstract = {Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-specific and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two significant problems in natural language processing-synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets confirm improved performance compared to the bag of words document representation.},
author = {Gabrilovich, E and Markovitch, S},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gabrilovich - 2007 - Harnessing the Expertise of 70,000 Human Editors Knowledge-Based Feature Generation for Text Categorization Shaul.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {background knowledge,feature generation,text classification},
number = {August 2005},
pages = {2297--2345},
title = {{Harnessing the expertise of 70,000 human editors: Knowledge-based feature generation for text categorization}},
url = {http://apps.isiknowledge.com.libproxy.unm.edu/full{\_}record.do?product=WOS{\&}colname=WOS{\&}search{\_}mode=RelatedRecords{\&}qid=29{\&}SID=1EDC8Ji5afddIhhlLPg{\&}page=1{\&}doc=1$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.6057{\&}rep=rep1{\&}type=pdf},
volume = {8},
year = {2007}
}
@article{Dappert2008,
abstract = {As institutions turn towards developing archival digital repositories, many decisions on the use of metadata have to be made. In addition to deciding on the more traditional descriptive and administrative metadata, particular care needs to be given to the choice of structural and preservation metadata, as well as to integrating the various metadata components. This article reports on the use of METS structural, PREMIS preservation and MODS descriptive metadata for the British Library's eJournal system.},
author = {Dappert, Angela and Enders, Markus},
doi = {10.1045/september2008-dappert},
isbn = {1082-9873 ST - Using METS, PREMIS and MODS for Archiving eJournals},
issn = {10829873},
journal = {D-Lib Magazine},
month = {sep},
number = {9-10},
title = {{Using METS, PREMIS and MODS for archiving eJournals}},
url = {http://www.dlib.org/dlib/september08/dappert/09dappert.html},
volume = {14},
year = {2008}
}
@article{Cheslow2014,
author = {Cheslow, S},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheslow - 2014 - METS For The Cultural Heritage Community A Literature Review.pdf:pdf},
journal = {Library Philosophy and Practice},
number = {1},
title = {{METS for the cultural heritage community: A literature review}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84938855325{\&}partnerID=40{\&}md5=0329e02c79db8a3a4d02fe97fe733182},
volume = {2014},
year = {2014}
}
@article{AlipourHafezi2013,
author = {{Alipour Hafezi}, M. and Horri, A. and Shiri, A. and Ghaebi, A.},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hafezi et al. - 2013 - Digital Library Interoperability Proposing a Model.pdf:pdf},
issn = {20088302},
journal = {International Journal of Information Science and Management},
keywords = {Digital libraries,Information system integration,Interoperability,Interoperability model,Iran,Metadata format,Protocols},
number = {1},
pages = {57--75},
title = {{Digital library interoperability: Proposing a model}},
volume = {11},
year = {2013}
}
@article{underwood2003xml,
abstract = {A three year Esprit project - ProCure - is ultimately aiming to take a significant but achievable step forward in the application of available information and communication technology (ICT) to the large scale engineering (LSE) construction industry. The ProCure consortium consists of five industrial partners supported by four associated research and expert partners. The project combines leading expertise from three member states to support ICT deployment by three industrial collaborative groups, i.e. UK, Germany and Finland. The basis of the project is in the partners' belief that sufficient ICT is now available to achieve deployment with care, in real projects, with an acceptable risk of failure. This paper presents work undertaken within the project to investigate the various metadata standards that exist in order to define a minimum metadata set based on these standards for the implementation of two demonstrators for XML based automated document exchange between a simulation of a corporate document management system and a simulation of a collaborative construction project Web site.},
author = {Underwood, J and Watson, A},
doi = {10.1108/09699980310466604},
isbn = {0969998031},
issn = {09699988 (ISSN)},
journal = {Engineering, Construction and Architectural Management},
keywords = {Collaboration,Colloboration,Computer simulation,Construction industry,Data structures,Electronic mail,Information analysis,Information exchange,Information exchanges,Information retrieval,Information technology strategies,Information technology strategy,Large scale engineering (LSE),Management information systems,Metadata,Project management,Web sites,Websites,XML},
number = {2},
pages = {128--145},
title = {{An XML metadata approach to seamless project information exchange between heterogeneous platforms}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-3242664144{\&}partnerID=40{\&}md5=a96ac20299e4b412af5193508dd7f539},
volume = {10},
year = {2003}
}
@article{Kules2006,
address = {Los Alamitos, CA, USA},
author = {Kules, Bill and Kustanowitz, Jack and Shneiderman, Ben},
doi = {http://doi.ieeecomputersociety.org/10.1145/1141753.1141801},
isbn = {1595933549},
journal = {Digital Libraries, Joint Conference on},
keywords = {browsing,categorization,classification,open,taxonomies},
pages = {210--219},
publisher = {IEEE Computer Society},
title = {{Categorizing Web Search Results into Meaningful and Stable Categories Using Fast-Feature Techniques}},
volume = {0},
year = {2006}
}
@article{schultze2000confessional,
abstract = {Information systems research has traditionally focused on information as an object that serves as input to decision making. Such a perspective attends mainly to the use of information. Increasingly, however, organizations are concerned about the production of information. This paper focuses on the work of producing informational objects, an activity central to knowledge work. Based on data collected during an eight-month ethnographic study of three groups of knowledge workers--computer system administrators, competitive intelligence analysts, and librarians--I explore the informing practices they relied upon. These are identified as ex-pressing, monitoring, and translating. Common to these informing practices is the knowledge workers' endeavor to balance subjectivity and objectivity, where subjectivity is a necessary part of doing value adding work and objectivity promises workers authority and a sense of security. Recognizing that researchers are knowledge workers too, I draw on my own experiences as an ethnographic researcher to identify parallels between my informing practices and those of the knowledge workers I studied. These parallels are intended to challenge the taken-for-granted assumptions underlying scientific practice. l adopt a confessional genre of representation for this purpose. [ABSTRACT FROM AUTHOR]},
author = {Schultze, Ulrike},
doi = {10.2307/3250978},
isbn = {02767783},
issn = {02767783},
journal = {MIS Quarterly},
keywords = {BUSINESS intelligence,COMPUTER systems,DECISION making,ETHNOLOGY,INFORMATION resources,INFORMATION resources management,INFORMATION technology,KNOWLEDGE management,KNOWLEDGE workers,LIBRARIANS,SOCIAL structure},
number = {1},
pages = {3--41},
pmid = {3205159},
publisher = {JSTOR},
title = {{A Confessional Account of an Ethnography About Knowledge Work}},
volume = {24},
year = {2000}
}
@article{Gartner201317,
abstract = {The article discusses a new XML schema, Parliamentary Metadata Language (PML), which has been devised to integrate diversely located records of legislative proceedings. The schema is designed to integrate with preexisting standards such as MODS and METS within an XML environment. It is discussed in the context of the LIPARM (Linking Parliamentary Records through Metadata) project within which it was devised: the project has also constructed a series of controlled vocabularies, encoded in MADS, to allow semantic integration across collections, and has designed a Web-based interface to PML records created from two substantial collections. Adapted from the source document.},
annote = {cited By 1},
author = {Gartner, Richard},
doi = {http://dx.doi.org/10.1080/19386389.2013.778728},
issn = {1938-6389, 1938-6389},
journal = {Journal of Library Metadata},
keywords = {12.11: CATALOGUING AND INDEXING,Integration,Law,Metadata,Parliaments,Standards,XML,article},
number = {1},
pages = {17--35},
title = {{Parliamentary Metadata Language: An XML Approach to Integrated Metadata for Legislative Proceedings}},
url = {http://search.proquest.com/docview/1417518839?accountid=17252$\backslash$nhttp://217.13.120.161:9004/uc3m?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}genre=article{\&}sid=ProQ:ProQ:lisashell{\&}atitle=Parliamentary+Metadata+Language:+An+XML+Approach+to+In},
volume = {13},
year = {2013}
}
@article{ISI:000209115700004,
abstract = {The use of the Metadata Encoding and Transmission Standard (METS) schema as a mechanism for delivering a digital library of complex scientific multimedia is examined as an alternative to the Fedora Content Model (FCM). Using METS as an "intermediary" schema, where it functions as a template that is populated with content metadata on the fly using Extensible Stylesheet Language Transformations (XSLT), it is possible to replicate the flexibility of structure and granularity of FCM while avoiding its complexity and often substantial demands on developers. [ABSTRACT FROM AUTHOR]},
author = {Gartner, Richard},
doi = {10.6017/ital.v31i3.1917},
issn = {0730-9295},
journal = {Information Technology and Libraries},
number = {3},
pages = {24--36},
title = {{METS as an 'intermediary' schema for a digital library of complex scientific multimedia}},
volume = {31},
year = {2012}
}
@article{McDonough2006148,
abstract = {METS is an XML document format intended for the encoding of complex objects within digital libraries. It provides the means to record all of the descriptive, admin- istrative, structural and behavioral metadata needed to man- age and provide access to complex digital content. While it was designed to promote interoperability of digital con- tent between digital library systems and contribute to the preservation of digital library materials, a variety of practical barriers to achieving these goals remain. However, many of these obstacles are shared by other communities of practice, such as the eLearning community working on the IMS con- tent packaging standards and the MPEG-21 community, and the digital library community faces a unique opportunity at themoment towork closely with others to try to improve the interoperability of our content not only with our own repos- itory systems, but those being used by others.},
annote = {cited By 13},
author = {McDonough, Jerome P.},
doi = {10.1007/s00799-005-0132-1},
issn = {14325012},
journal = {International Journal on Digital Libraries},
keywords = {Digital preservation,Interoperability,METS,Structural metadata,XML},
number = {2},
pages = {148--158},
title = {{METS: Standardized encoding for digital library objects}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-33645686128{\{}{\{}{\}}{\{}{\&}{\}}{\{}{\}}{\}}partnerID=40{\{}{\{}{\}}{\{}{\&}{\}}{\{}{\}}{\}}md5=c166ed736cf6347d72fd0ad9c551b36e},
volume = {6},
year = {2006}
}
@article{ISI:000180545300005,
abstract = {The optical properties of metal nanoparticles have long been of interest in physical chemistry, starting with Faraday's investigations of colloidal gold in the middle 1800s. More recently, new lithographic techniques as well as improvements to classical wet chemistry methods have made it possible to synthesize noble metal nanoparticles with a wide range of sizes, shapes, and dielectric environments. In this feature article, we describe recent progress in the theory of nanoparticle optical properties, particularly methods for solving Maxwell's equations for light scattering from particles of arbitrary shape in a complex environment. Included is a description of the qualitative features of dipole and quadrupole plasmon resonances for spherical particles; a discussion of analytical and numerical methods for calculating extinction and scattering cross-sections, local fields, and other optical properties for nonspherical particles; and a survey of applications to problems of recent interest involving triangular silver particles and related shapes.},
address = {1155 16TH ST, NW, WASHINGTON, DC 20036 USA},
author = {Kelly, K. Lance and Coronado, Eduardo and Zhao, Lin Lin and Schatz, George C.},
doi = {10.1021/jp026731y},
isbn = {1520-6106},
issn = {10895647},
journal = {Journal of Physical Chemistry B},
number = {3},
pages = {668--677},
pmid = {11230091},
publisher = {AMER CHEMICAL SOC},
title = {{The optical properties of metal nanoparticles: The influence of size, shape, and dielectric environment}},
type = {Article},
volume = {107},
year = {2003}
}
@article{Martinez-Cruz2011,
abstract = {Twomain datamodels are currently used for representing knowledge and infor- mation in computer systems. Database models, especially relational databases, have been the leader in last few decades, enabling information to be efficiently stored and queried. On the other hand, ontologies have appeared as an alternative to databases in applications that require a more ‘enriched' meaning. However, there is controversy regarding the best infor- mation modeling technique, as both models present similar characteristics. In this paper, we present a review of how ontologies and databases are related, of what their main differences are and of the mechanisms used to communicate with each other.},
author = {Martinez-Cruz, Carmen and Blanco, Ignacio J. and Vila, M. Amparo},
doi = {10.1007/s10462-011-9251-9},
isbn = {0269-2821},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Ontologies,Ontology-based databases,Relational databases},
number = {4},
pages = {271--290},
title = {{Ontologies versus relational databases: Are they so different? A comparison}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84870244777{\{}{\&}{\}}partnerID=tZOtx3y1},
volume = {38},
year = {2012}
}
@article{DBLP:journals/jis/NaT09,
abstract = {The motivation of this study is to enhance general topical search$\backslash$nwith a sentiment-based one where the search results (snippets) returned$\backslash$nby the web search engine are clustered by sentiment categories. Firstly$\backslash$nwe developed an automatic method to identify product review documents$\backslash$nusing the snippets (summary information that includes the URL, title,$\backslash$nand summary text), which is genre classification. Then the identified$\backslash$nsnippets were automatically classified into positive (recommended)$\backslash$nand negative (non-recommended) documents, which is sentiment classification.$\backslash$nThereafter the user may directly decide to access the positive or$\backslash$nnegative review documents. In this study we used only the snippets$\backslash$nrather than their original full-text documents, and applied a common$\backslash$nmachine learning technique, SVM (support vector machine), and heuristic$\backslash$napproaches to investigate how effectively the snippets can be used$\backslash$nfor genre and sentiment classification. The results show that the$\backslash$nweb search engine should improve the quality of the snippets especially$\backslash$nfor opinionated documents (i.e. review documents). {\textcopyright} CILIP.},
author = {{Na J.-C.}, Thet T T},
doi = {10.1177/0165551509104233},
issn = {0165-5515},
journal = {Journal of Information Science},
keywords = {genre classification,product review documents,sentiment classification,snippets,web},
number = {6},
pages = {709--726},
title = {{Effectiveness of web search results for genre and sentiment classification}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70849127072{\&}partnerID=40{\&}md5=9a488bca2647ca72037d78f9b1b3e25a},
volume = {35},
year = {2009}
}
@article{barker2010metadata,
author = {Barker, Phil and Campbell, Lorna M},
journal = {Technology, Instruction, Cognition, and Learning},
number = {3-4},
pages = {225--243},
title = {{Metadata for learning materials: an overview of existing standards and current developments}},
volume = {7},
year = {2010}
}
@article{Ramos-SotoBBT14,
abstract = {It is well known that the fundamental intellectual problems of information access are the production and consumption of information.$\backslash$n  In this paper, we investigate the use of social network of information producers (authors) within relations in data (co-authorship$\backslash$n  and citation) in order to improve the relevance of information access. Relevance is derived from the network by levraging$\backslash$n  the usual topical similarity between the query and the document with the target author's authority. We explore various social$\backslash$n  network based measures for computing social information importance and show how this kind of contextual information can be$\backslash$n  incorporated within an information access model. We experiment with a collection issued from SIGIR proceedings and show that$\backslash$n  combining topical, author and citation based evidences can significantly improve retrieval access precision, measured in terms$\backslash$n  of mean reciprocal rank.},
author = {Tamine, Lynda and Jabeur, Amjed and Bahsoun, Wahiba},
doi = {10.1007/978-3-642-04957-6},
isbn = {978-3-642-04956-9},
issn = {03029743},
journal = {Flexible Query Answering Systems},
number = {JANUARY},
pages = {88--98},
title = {{Flexible Query Answering Systems}},
url = {http://www.springerlink.com/content/0x25x70r8478t1n8},
volume = {5822},
year = {2009}
}
@inproceedings{dave2003mining,
abstract = {The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.},
author = {Dave, Kushal and Lawrence, Steve and Pennock, David M.},
booktitle = {Proceedings of the twelfth international conference on World Wide Web - WWW '03},
doi = {10.1145/775152.775226},
isbn = {1581136803},
issn = {12709638},
keywords = {as well as in,discussion boards and mailing,document classification,google groups,list archives,opinion mining,products in their personal,usenet via,users also comment on},
organization = {ACM},
pages = {519},
title = {{Mining the peanut gallery}},
url = {http://portal.acm.org/citation.cfm?doid=775152.775226},
year = {2003}
}
@article{aramossoto2016onthe,
abstract = {This paper explores the current state of the task of generating easily understandable information from data for people using natural language, which is currently addressed by two independent research fields: the natural language generation field — and, more specifically, the data-to-text sub-field — and the linguistic descriptions of data field. Both approaches are explained in a detailed description which includes: i) a methodological revision of both fields including basic concepts and definitions, models and evaluation procedures; ii) the most relevant systems, use cases and real applications described in the literature. Some reflections about the current state and future trends of each field are also provided, followed by several remarks that conclude by hinting at some potential points of mutual interest and convergence between both fields.},
author = {Ramos-Soto, A. and Bugar{\'{i}}n, A. and Barro, S.},
doi = {10.1016/j.fss.2015.06.019},
editor = {Mar{\'{i}}n, Nicol{\'{a}}s and S{\'{a}}nchez, Daniel},
issn = {01650114},
journal = {Fuzzy Sets and Systems},
pages = {1--21},
publisher = {ELSEVIER SCIENCE BV},
title = {{On the role of linguistic descriptions of data in the building of natural language generation systems}},
volume = {285},
year = {2015}
}
@inproceedings{pustejovsky2003timebank,
abstract = {While the mechanisms for conveying temporal information in language have been have been extensively studied by linguists, very little of this work has been done in the tradition of corpus linguistics. In this paper we discuss the outcomes of a research effort to build a corpus, called TIMEBANK, which is richly annotated to indicate events, times, and temporal relations. We describe the annotation scheme, the corpus sources and tools used in the annotation process, and then report some preliminary figures about the distribution of various phenomena across the corpus. TIMEBANK represents the most fine-grained and extensively temporally annotated corpus to date, and will be a valuable resource both for corpus linguists interested in time and language, and for language engineers interested in applications such as question answering and information extraction for which accurate knowledge of the position and ordering of events in time is of key importance.},
author = {Pustejovsky, James and Hanks, Patrick and Saur{\'{i}}, Roser and See, Andrew and Gaizauskas, Robert and Setzer, Andrea and Radev, Dragomir and Sundheim, Beth and Day, David and Ferro, Lisa},
booktitle = {Corpus linguistics},
pages = {40},
title = {{The TIMEBANK Corpus}},
volume = {2003},
year = {2003}
}
@article{duval2002metadata,
abstract = {The rapid changes in the means of information access occasioned by the emergence of the World Wide Web have spawned an upheaval in the means of describing and managing information resources. Metadata is a primary tool in this work, and an important link in the value chain of knowledge economies. Yet there is much confusion about how metadata should be integrated into information systems. How is it to be created or extended? Who will manage it? How can it be used and exchanged? Whence comes its authority? Can different metadata standards be used together in a given environment? These and related questions motivate this paper. The authors hope to make explicit the strong foundations of agreement shared by two prominent metadata Initiatives: the Dublin Core Metadata Initiative (DCMI) and the Institute for Electrical and Electronics Engineers (IEEE) Learning Object Metadata (LOM) Working Group.... The ideas in this paper are divided into two categories. Principles are those concepts judged to be common to all domains of metadata and which might inform the design of any metadata schema or application. Practicalities are the rules of thumb, constraints, and infrastructure issues that emerge from bringing theory into practice in the form of useful and sustainable systems.},
author = {Duval, Erik and Hodgins, Wayne and Sutton, Stuart and Weibel, Stuart L.},
doi = {10.1045/april2002-weibel},
isbn = {1082-9873},
issn = {10829873},
journal = {D-Lib Magazine},
number = {4},
pages = {16},
title = {{Metadata principles and practicalities}},
volume = {8},
year = {2002}
}
@article{dempsey1997specification,
abstract = {This study provides background information to the DESIRE project to enable the implications of using particular metadata formats to be assessed. Part I is a brief introductory review of issues including consideration of the environment of use and the characteristics of metadata formats. A broad typology of metadata is introduced to provide a framework for analysis. Part II consists of an outline of resource description formats in directory style. This includes generic formats, but also, to give an indication of the range of development, domain-specific formats. The focus is on metadata for 'information resources' broadly understood rather than on the variety of other approaches which exist within particular scientific, engineering and other areas.},
author = {Dempsey, Lorcan and Computer, Online},
number = {October},
publisher = {DESIRE project},
title = {{Specification for resource description methods , Part 1 . A review of metadata : a survey of current resource description formats}},
url = {http://opus.bath.ac.uk/23579/},
year = {2015}
}
@inproceedings{chen2003developing,
author = {Chen, Ruey-Shun and Yu, Shien-Chiang},
booktitle = {ISICT '03: Proceedings of the 1st international symposium on Information and communication technologies},
keywords = {dtd,metadata,schema,system design,xml},
organization = {Trinity College Dublin},
pages = {267--272},
title = {{Developing an XML framework for metadata system}},
year = {2003}
}
@article{heery1996review,
author = {Heery, Rachel},
doi = {10.1108/eb047236},
issn = {0033-0337},
journal = {Program: electronic library and information systems},
number = {4},
pages = {345--373},
publisher = {MCB UP Ltd},
title = {{Review of metadata formats}},
url = {http://www.researchgate.net/publication/238308456{\_}Review{\_}of{\_}metadata{\_}formats},
volume = {30},
year = {1996}
}
@article{Dulock2009289,
abstract = {The University of Colorado at Boulder recently engaged in a grant-funded pilot project to use Metadata Encoding {\&} Transmission Standard ({\{}METS){\}}, Metadata Object Description Standard ({\{}MODS){\}}, and {\{}NISO{\}} Metadata for Images in {\{}XML{\}} Schema ({\{}MIX){\}} for a collection of digitized Sanborn fire insurance maps of the state. This article will draw on this experience to outline the processes and decision making required to implement new metadata structures, and will offer some insights on planning strategically for an institution's first use of these increasingly important metadata standards. [{\{}ABSTRACT{\}} {\{}FROM{\}} {\{}AUTHOR]{\}}},
annote = {cited By 2},
author = {Dulock, Michael and Cronin, Christopher},
doi = {10.1080/19386380903405199},
issn = {1938-6389},
journal = {Journal of Library Metadata},
number = {February 2015},
pages = {289--304},
title = {{Providing Metadata for Compound Digital Objects: Strategic Planning for an Institution's First Use of METS, MODS, and MIX}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-79951906211{\{}{\&}{\}}partnerID=40{\{}{\&}{\}}md5=32ac82ccb9f4680378715974c7243de6},
volume = {9},
year = {2009}
}
@article{Blascheck2016,
abstract = {Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.},
author = {Blascheck, Tanja and John, Markus and Kurzhals, Kuno and Koch, Steffen and Ertl, Thomas},
doi = {10.1109/TVCG.2015.2467871},
isbn = {1077-2626 VO - 22},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Data visualization,Gaze tracking,Navigation,Protocols,Synchronization,Visual analytics},
month = {jan},
number = {1},
pages = {61--70},
pmid = {26529687},
publisher = {IEEE Computer Society},
title = {{VA2: A Visual Analytics Approach for // Evaluating Visual Analytics Applications}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84946616161{\&}partnerID=tZOtx3y1},
volume = {22},
year = {2016}
}
@article{Szpunar2010,
abstract = {The ability to mentally simulate hypothetical scenarios is a rapidly growing area of research in both psychology and neuroscience. Episodic future thought, or the ability to simulate specific personal episodes that may potentially occur in the future, represents one facet of this general capacity that continues to garner a considerable amount of interest. The purpose of this article is to elucidate current knowledge and identify a number of unresolved issues regarding this specific mental ability. In particular, this article focuses on recent research findings from neuroimaging, neuropsychology, and clinical psychology that have demonstrated a close relation between episodic future thought and the ability to remember personal episodes from one's past. On the other hand, considerations of the role of abstracted (semantic) representations in episodic future thought have been noticeably absent in the literature. The final section of this article proposes that both episodic and semantic memory play an important role in the construction of episodic future thoughts and that their interaction in this process may be determined by the relative accessibility of information in memory. {\textcopyright} The Author(s) 2010.},
author = {Szpunar, K. K.},
doi = {10.1177/1745691610362350},
isbn = {17456916 (ISSN)},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {Episodic future thought,Episodic memory,Mental simulation,Mental time travel,away from the,direct one,episodic future thought,episodic memory,is the ability to,mental simulation,mental time travel,most fascinating features of,perhaps one of the,s attention inward,the human mind},
month = {mar},
number = {2},
pages = {142--162},
pmid = {26162121},
title = {{Episodic future thought: An emerging concept}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77955847484{\&}partnerID=40{\&}md5=7ff3291154f53c343f57ea679f0fe2d6$\backslash$nhttp://pps.sagepub.com/content/5/2/142$\backslash$nhttp://pps.sagepub.com/content/5/2/142.full.pdf},
volume = {5},
year = {2010}
}
@article{Du2013,
abstract = {With the Internet growing exponentially, search engines are encountering unprecedented challenges. A focused search engine selectively seeks out web pages that are relevant to user topics. Determining the best strategy to utilize a focused search is a crucial and popular research topic. At present, the rank values of unvisited web pages are computed by considering the hyperlinks (as in the PageRank algorithm), a Vector Space Model and a combination of them, and not by considering the semantic relations between the user topic and unvisited web pages. In this paper, we propose a concept context graph to store the knowledge context based on the user's history of clicked web pages and to guide a focused crawler for the next crawling. The concept context graph provides a novel semantic ranking to guide the web crawler in order to retrieve highly relevant web pages on the user's topic. By computing the concept distance and concept similarity among the concepts of the concept context graph and by matching unvisited web pages with the concept context graph, we compute the rank values of the unvisited web pages to pick out the relevant hyperlinks. Additionally, we constitute the focused crawling system, and we retrieve the precision, recall, average harvest rate, and F-measure of our proposed approach, using Breadth First, Cosine Similarity, the Link Context Graph and the Relevancy Context Graph. The results show that our proposed method outperforms other methods. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Du, Yajun and Pen, Qiangqiang and Gao, Zhaoqiong},
doi = {10.1016/j.datak.2013.09.003},
isbn = {1398009725},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Concept context graph,Focused crawling,Formal concept analysis,Information retrieval,Search engine,Web crawler,Web information systems},
month = {nov},
pages = {75--93},
title = {{A topic-specific crawling strategy based on semantics similarity}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84889100636{\&}partnerID=tZOtx3y1},
volume = {88},
year = {2013}
}
@article{Tague1981,
abstract = {Discusses the notion that knowledge grows exponentially and describes its growth mathematically by an exponential function. Growth patterns in subfields of knowledge or research areas are described citing related research. Interpretation of growth rate statistics and forecasts are included. Thirty-three references and statistics used for graphs are appended. (EJS)},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Tague, Jean and Beheshti, Jamshid and Rees-Potter, Lorna},
doi = {10.1080/19338244.2010.483622},
eprint = {0521865719 9780521865715},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tague, Beheshti, Rees-Potter - Unknown - The Law of Exponential Growth Evidence, Implications and Forecasts(4).pdf:pdf},
isbn = {0028-0836},
issn = {00242594},
journal = {Library Trends},
number = {1},
pages = {125--149},
pmid = {20705579},
title = {{The Law of Exponential Growth: Evidence, Implications and Forecasts}},
volume = {30},
year = {1981}
}
@article{Tague1981a,
abstract = {Discusses the notion that knowledge grows exponentially and describes its growth mathematically by an exponential function. Growth patterns in subfields of knowledge or research areas are described citing related research. Interpretation of growth rate statistics and forecasts are included. Thirty-three references and statistics used for graphs are appended. (EJS)},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Tague, Jean and Beheshti, Jamshid and Rees-Potter, Lorna},
doi = {10.1080/19338244.2010.483622},
eprint = {0521865719 9780521865715},
file = {:C$\backslash$:/Users/eric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tague, Beheshti, Rees-Potter - Unknown - The Law of Exponential Growth Evidence, Implications and Forecasts(4).pdf:pdf},
isbn = {0028-0836},
issn = {00242594},
journal = {Library Trends},
number = {1},
pages = {125--149},
pmid = {20705579},
title = {{The Law of Exponential Growth: Evidence, Implications and Forecasts}},
volume = {30},
year = {1981}
}
@INPROCEEDINGS{4028513, 
author={Y. f. Wang and Y. j. Zhang and Z. t. Xu and T. Zhang}, 
booktitle={2006 International Conference on Machine Learning and Cybernetics}, 
title={Research on Dual Pattern of Unsupervised and Supervised Word Sense Disambiguation}, 
year={2006}, 
pages={2665-2669}, 
abstract={As an important work in the field of natural language processing, word sense disambiguation (WSD) has been a research focus since 1950. The task of WSD is very difficult to solve, and most of modern algorithms fail to reach an ideal level. The processing for WSD is to determine the sense of a polysemous word within a specific context, which involves two steps - determining all the senses for the polysemous word and selecting the appropriate sense among them. In this paper, a dual pattern of WSD based on supervised and unsupervised learning is proposed. Hence, WSD problem can be solved under different circumstances and conditions. Also, an adapted extended Lesk algorithm is established. The experiment results show that the whole quality of unsupervised and supervised WSD is satisfactory}, 
keywords={dictionaries;natural languages;support vector machines;unsupervised learning;word processing;adapted extended Lesk algorithm;natural language processing;polysemous word;supervised learning;support vector machine;unsupervised learning;word sense disambiguation;Clustering algorithms;Computer science;Cybernetics;Dictionaries;Machine learning;Machine learning algorithms;Natural language processing;Supervised learning;Support vector machine classification;Support vector machines;Training data;Unsupervised learning;Support Vector Machine;Word Sense Disambiguation;WordNet;supervised learning;unsupervised learning}, 
doi={10.1109/ICMLC.2006.258922}, 
ISSN={2160-133X}, 
month={Aug},}

@INPROCEEDINGS{5494927, 
author={H. Sak and M. Saraçlar and T. Güngör}, 
booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
title={Morphology-based and sub-word language modeling for Turkish speech recognition}, 
year={2010}, 
pages={5402-5405}, 
keywords={speech recognition;Turkish broadcast news transcription;Turkish speech recognition;automatic speech recognition;finite-state transducer;lexical morphemes;morphological disambiguation;morphological parser;morphologically rich languages;morphology-based language modeling;morphology-based model;morphology-integrated model;morphology-integrated search network;morphosyntactic features;morphotactics disambiguation;semantic features;subword language modeling;word error rate;Automatic speech recognition;Broadcasting;Encoding;Error analysis;Lattices;Natural languages;Parameter estimation;Speech recognition;Transducers;Vocabulary;Morphology-based;automatic speech recognition;language modeling;morphology-integrated;sub-word}, 
doi={10.1109/ICASSP.2010.5494927}, 
ISSN={1520-6149}, 
month={March},}

@INPROCEEDINGS{6982457, 
author={R. Creţulescu and A. David and D. Morariu and L. Vinţan}, 
booktitle={System Theory, Control and Computing (ICSTCC), 2014 18th International Conference}, 
title={Part of speech tagging with Na #x00EF;ve Bayes methods}, 
year={2014}, 
pages={446-451}, 
keywords={natural language processing;statistical analysis;Brown University Standard Corpus of Present;POS labeling;backward method;complete method;context awareness statistic methods;forward method;naive Bayes methods;part-of-speech tagging;parts-of-speech prediction;Accuracy;Bayes methods;Context;Measurement;Speech;Tagging;Training;NLP;Naïve Bayes;Part of Speech Prediction}, 
doi={10.1109/ICSTCC.2014.6982457}, 
month={Oct},}

@INPROCEEDINGS{5599823, 
author={Y. Guo and W. Che and T. Liu and S. Li}, 
booktitle={Cognitive Informatics (ICCI), 2010 9th IEEE International Conference on}, 
title={Semi-supervised domain adaptation for WSD: Using a word-by-word model selection approach}, 
year={2010}, 
pages={680-687}, 
keywords={learning (artificial intelligence);natural language processing;natural language processing;self-training models;semi-supervised domain adaptation;supervised model;word sense disambiguation;word-by-word model selection;Accuracy;Adaptation model;Data models;Finance;Support vector machines;Training;Training data}, 
doi={10.1109/COGINF.2010.5599823}, 
month={July},}
