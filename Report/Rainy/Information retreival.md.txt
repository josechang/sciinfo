All information retreival  data is recorded.


Similarity threshold
The similarity threshold is a lower limit for the similarity of two data records that belong to the same cluster. 
For example, if you set the similarity threshold to 0.25, data records with field values that are 25% similar are likely to be assigned to the same cluster.
If you specify a similarity threshold of 1.0, you are insisting that, for customers to appear in the same group, their characteristics must be identical.
You might have a large number of customer characteristic variables, or the variables might take a wide range of values.
The right setting is somewhere in between, but where? You have to preserve a balance between the number of clusters that is acceptable and the degree of similarity.
There is another important factor that the Distribution-based Clustering algorithm has to consider.
A situation might occur where you try to give the Distribution-based Clustering algorithm a similarity threshold,and you do not limit the number of clusters that it can produce.
In this case, it will keep trying to find the minimum number of clusters that satisfy the similarity threshold, because this also maximizes the Condorcet value.
In a different situation, you might have limited the number of clusters with the result that, after all the possible clusters are created, a record does not have a similarity above the threshold with any of them.
In this case, the record will be assigned to the cluster with the best similarity, even if the similarity threshold is not reached.


Distribution-based Clustering
Distribution-based Clustering provides fast and natural clustering of very large databases.
It automatically determines the number of clusters to be generated.
Typically, demographic data consists of large amounts of categorical variables. 
Therefore the mining function works best with data sets that consist of this type of variables.
You can also use numerical variables. 
The Distribution-based Clustering algorithm treats numerical variables almost like categorical variables by categorizing their values into buckets.
Distribution-based Clustering is an iterative process over the input data. 
Each input record is read in succession. 
The similarity of each record with each of the currently existing clusters is calculated. 
Initially, no clusters exist. 
If the biggest calculated similarity is above a given threshold, the record is added to the relevant cluster. 
This cluster's characteristics change accordingly.
If the calculated similarity is not above the threshold, or if there is no cluster, a new cluster is created, containing the record alone.
You can specify the maximum number of clusters, as well as the similarity threshold.
Distribution-based Clustering uses the statistical Condorcet criteria to manage the calculation of the similarity between records and other records, between records and clusters, and between clusters and other clusters. 
The Condorcet criteria evaluates how homogeneous each discovered cluster is (in that the records it contains are similar) and how heterogeneous the discovered clusters are among each other. 
The iterative process of discovering clusters stops after one or more passes over the input data if there is no time remaining to do another pass or if the improvement of the clusters according to the Condorcet criteria would not justify a new pass.

Similarity scale
The similarity between two data records sums the similarities between each pair of values of these records.
For a categorical field, the similarity is 0 if both values are different, and 1 if they are equal.
For a numerical field, the similarity depends on the difference between the two values compared with the similarity scale of the field. 
If the two values are equal, the similarity is 1. If the two values are distant from the similarity scale, the similarity is 0.5. 
Other values of the similarity are calculated using a Gaussian curve passing through these two reference points.
You can choose to specify a similarity scale or not. 
You specify the similarity scale as an absolute number. The specification is considered only for active numerical fields. If you do not specify a similarity scale, the default value (half of the standard deviation) is used.

Similarity matrices
For each categorical field, you can define a similarity matrix that contains user-defined similarities between two field values.
A similarity matrix is represented as a reference to a database table containing three columns. 
Two columns contain the field values to be compared, and the third column contains the similarity (between 0 and 1) for these field values.
You can specify the similarity for each pair of possible values. 
A pair of possible values can be used only once. 
The similarity of the inverse pair is the same

Field weighting
Field weighting gives more or less weight to certain input fields during a Clustering training run.
For example, to identify different types of shoppers, you might not want to give too much weight to the strong correlation between the number of purchases and the total purchase amount. 
Therefore you assign a smaller weight to the fields Number of purchases and Total purchase amount.
The following table shows the decreased field weights for the fields Number of purchases and the Total purchase amount.

Value weighting
Value weighting deals with the fact that particular values in a field might be more common than other values in that field. 
The coincidence of rare values in a field adds more to the overall similarity than the coincidence of frequent values.
For example, most people do not have a Gold credit card. 
It is not very significant if two people do not have one, however, if they do, it is significant. 
Therefore, the coincidence of people not having a Gold credit card adds less to their overall similarity than the coincidence of people having one.
You can use one of the following types of value weighting:

Probability weighting
Probability weighting assigns a weight to each value according to its probability in the input data. Rare values carry a large weight, while common values carry a small weight. This weight is used for both matching and non-matching records.
Probability weighting uses a factor of 1/√p, where p is the probability of a value.

Logarithmic weighting
Logarithmic weighting assigns a weight to each value according to the logarithm of its probability in the input data.
 Rare values carry a large weight, while common values carry a small weight. 
This weight is used for both matching and non-matching records.
Logarithmic weighting assigns a value of √(-log(p)) to both the agreement information content value and the disagreement information content value. 
The number p is the probability of a value.
Each type of value weighting looks at a problem from a different angle.
Depending on the value distribution, using one type or the other might lead to very different results. 
For example, if a supermarket is located in a retirement community, a Senior discount field has a high probability of having the value Yes. 
You might use probabilistic value weighting to assign a weight to the values in the Senior discount field that is equal to its probability in the input data.
Value weighting has the additional effect of emphasizing fields with many values because their values are less frequent than those of fields with fewer possible values. 
By default, the mining function does not compensate for this additional effect.
You can select whether you want to compensate for the value weighting applied to each field. 
If you compensate for value weighting, the overall importance of the weighted field is equal to that of an unweighted field. 
This is so regardless of the number of possible values. 
Compensated weighting affects only the relative importance of coincidences within the set of possible values.

Maximum number of distribution-based clusters
You can control the number of clusters to be created during a Clustering training run by specifying a value for the maximum number of clusters.
 Limiting the number of clusters prevents the production of many small clusters, and thus saves run time.
By default, the Distribution-based Clustering algorithm does not create an unlimited number of clusters but sets itself a limit.
If the mining function cannot continue to create clusters because the cluster limit is reached, this may prevent further improvement in the accuracy of the model.
Increasing the number of clusters improves the likelihood of finding market niches.

UNIFIED MODELING LANGUAGE
Introduction to OMG UML by Dr. Jon Siegel, OMG
Large enterprise applications-the ones that execute core business applications, and keep a company going-must be more than just a bunch of code modules. 
They must be structured in a way that enables scalability, security, and robust execution under stressful conditions, and their structure - frequently referred to as their architecture - must be defined clearly enough that maintenance programmers can (quickly!). 
find and fix a bug that shows up long after the original authors have moved on to other projects. 
That is, these programs must be designed to work perfectly in many areas, and business functionality is not the only one (although it certainly is the essential core).
Of course a well-designed architecture benefits any program, and not just the largest ones as we've singled out here. 
We mentioned large applications first because structure is a way of dealing with complexity.
so the benefits of structure (and of modeling and design, as we'll demonstrate) compound as application size grows large. 
Another benefit of structure is that it enables code reuse: Design time is the easiest time to structure an application as a collection of self-contained modules or components.
Eventually, enterprises build up a library of models of components, each one representing an implementation stored in a library of code modules. 
When another application needs the same functionality, the designer can quickly import its module from the library. 
At coding time, the developer can just as quickly import the code module into the application. 

Modeling is the designing of software applications before coding. 
Modeling is an Essential Part of large software projects, and helpful to medium and even small projects as well. 
A model plays the analogous role in software development that blueprints and other plans (site maps, elevations, physical models) play in the building of a skyscraper.
Using a model, those responsible for a software development project's success can assure themselves that business functionality is complete and correct, end-user needs are met, and program design supports requirements for scalability, robustness, security, extendibility, and other characteristics, before implementation in code renders changes difficult and expensive to make.
Surveys show that large software projects have a huge probability of failure - in fact, it's more likely that a large software application will fail to meet all of its requirements on time and on budget than that it will succeed. 
If you're running one of these projects, you need to do all you can to increase the odds for success, and modeling is the only way to visualize your design and check it against requirements before your crew starts to code.

YOU CAN DO OTHER USEFUL THINGS WITH UML TOO
some tools analyze existing source code (or, some claim, object code!) and reverse-engineer it into a set of UML diagrams. 
Another example: Some tools on the market execute UML models, typically in one of two ways: Some tools execute your model interpretively in a way that lets you confirm that it really does what you want, but without the scalability and speed that you'll need in your deployed application. 
Other tools (typically designed to work only within a restricted application domain such as telecommunications or finance) generate program language code from UML, producing most of a bug-free, deployable application that runs quickly if the code generator incorporates best-practice scalable patterns for, e.g., transactional database operations or other common program tasks. 
(OMG members are working on a specification for Executable UML now.) 
Our final entry in this category: A number of tools on the market generate Test and Verification Suites from UML models. 

MODELS VS. METHODOLOGIES
The process of gathering and analyzing an application's requirements, and incorporating them into a program design, is a complex one and the industry currently supports many methodologies that define formal procedures specifying how to go about it. 
One characteristic of UML - in fact, the one that enables the widespread industry support that the language enjoys - is that it is methodology-independent. 
Regardless of the methodology that you use to perform your analysis and design, you can use UML to express the results. 
And, using XMI® (XML™ Metadata Interchange, another OMG standard), you can transfer your UML model from one tool into a repository, or into another tool for refinement or the next step in your chosen development process. 
These are the benefits of standardization!.

WHAT CAN YOU MODEL WITH UML?
UML 2.0 defines thirteen types of diagrams, divided into three categories: Six diagram types represent static application structure; three represent general types of behavior; and four represent different aspects of interactions:
Structure Diagrams:  include the Class Diagram, Object Diagram, Component Diagram, Composite Structure Diagram, Package Diagram, and Deployment Diagram.
Behavior Diagrams:  include the Use Case Diagram (used by some methodologies during requirements gathering); Activity Diagram, and State Machine Diagram. 
Interaction Diagrams:  all derived from the more general Behavior Diagram, include the Sequence Diagram, Communication Diagram, Timing Diagram, and Interaction Overview Diagram.
  We don't intend this introductory web page to be a complete UML tutorial, so we're not going to list any details of the different diagram types here. 
To learn more, you can check out one of the many on-line tutorials, or buy a book. (The last time we checked, typing "UML" into the search box for the major on-line booksellers returned a list of more than 100  titles!) Or, if you're technical and want the whole story, you can download the UML specification itself from the OMG website. 
It's free, of course, but it's also highly technical, terse, and very difficult for beginners to understand. 

I'M ABOUT TO START MY FIRST UML-BASED DEVELOPMENT PROJECT. WHAT DO I NEED TO DO?
Three things, probably (but not necessarily) in this order:
1)Select a methodology: A methodology formally defines the process that you use to gather requirements, analyze them, and design an application that meets them in every way. 
There are many methodologies, each differing in some way or ways from the others. 
There are many reasons why one methodology may be better than another for your particular project: For example, some are better suited for large enterprise applications while others are built to design small embedded or safety-critical systems. 
On another axis, some methods better support large numbers of architects and designers working on the same project, while others work better when used by one person or a small group
OMG, as a vendor-neutral organization, does not have an opinion about any methodology.

2)Select a UML Development Tool: Because most (although not all) UML-based tools implement a particular methodology, in some cases it might not be practical to pick a tool and then try to use it with a methodology that it wasn't built for. (For other tool/methodology combinations, this might not be an issue, or might be easy to work around.) 
But, some methodologies have been implemented on multiple tools so this is not strictly a one-choice environment.
You may find a tool so well-suited to your application or organization that you're willing to switch methodologies in order to use it. If that's the case, go ahead - our advice to pick a methodology first is general, and may not apply to a specific project. Another possibility: You may find a methodology that you like, which isn't implemented in a tool that fits your project size, or your budget, so you have to switch. If either of these cases happens to you, try to pick an alternative methodology that doesn't differ too much from the one you preferred originally. 
As with methodologies, OMG doesn't have an opinion or rating of UML-based modeling tools, but we do have links to a number of lists here. These will help you get started making your choice. 
You may find a tool so well-suited to your application or organization that you're willing to switch methodologies in order to use it. 
If that's the case, go ahead - our advice to pick a methodology first is general, and may not apply to a specific project. 
Another possibility: You may find a methodology that you like, which isn't implemented in a tool that fits your project size, or your budget, so you have to switch. 
If either of these cases happens to you, try to pick an alternative methodology that doesn't differ too much from the one you preferred originally. 
As with methodologies, OMG doesn't have an opinion or rating of UML-based modeling tools, but we do have links to a number of lists here. 
These will help you get started making your choice.

3)Get Training: You and your staff (unless you're lucky enough to hire UML-experienced architects) will need training in UML. 
It's best to get training that teaches how to use your chosen tool with your chosen methodology, typically provided by either the tool supplier or methodologist. 
If you decide not to go this route, check out OMG's training page for a course that meets your needs. Once you've learned UML, you can become an OMG-certified UML Professional-check here for details.
As with methodologies, OMG doesn't have an opinion or rating of UML-based modeling tools, but we do have links to a number of lists here. 
These will help you get started making your choice. 